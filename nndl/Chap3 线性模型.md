# 线性模型

+ logistic回归
+ softmax回归
+ 感知器
+ SVM

## 1.线性回归

​	线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。

​	单层神经网络

​	模型训练：通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。

#### 优化算法

​	大多数深度学习模型并没有解析解（直接用公式就可以表达），只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解。

###### 小批量随机梯度下降：

​	a.先选取一组模型参数的初始值，如随机选取；

​	b.接下来对参数进行多次迭代，目的是使每次迭代都可能降低损失函数的值。

​	c.在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数（学习率）的乘积作为模型参数在本次迭代的减小量。

![1571110544549](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571110544549.png)

B表示样本容量

调参：调超参数

使用矢量计算，提高效率

![1571122673080](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571122673080.png)

## 2.softmax回归

​	softmax回归则适用于分类问题。分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。

​	softmax运算将输出转换成一个合法的概率分布，输出个数=分类问题中的类别个数。实际上，真实标签也可以用类别分布表达，这样我们的训练目标可以设为使预测概率分布尽可能接近真实的标签概率分布。

​	单层神经网络

#### softmax运算

![1571124499867](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571124499867.png)

#### 熵

在信息论中，熵用来衡量一个随机事件的不确定性。

自信息:

​					 I(x) = -log⁡(p(x))

熵：

![1570952218361](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1570952218361.png)

交叉熵：衡量两个概率分布的差异，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。



![1570952299547](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1570952299547.png)

KL散度是用概率分布q来近似p时所造成的信息损失量

## 3.感知器

​	隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。

​	多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换

​	多层感知机按以下方式计算输出：

![1571211914836](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571211914836.png)

## 4.SVM

（1）支持向量机（Support Vector Machine，SVM）是一个经典二类分类算法。

​	给定一个二分类器数据集D ={(x(n),y(n))}N n=1，其中yn ∈{+1,−1}，如果两类样本是线性可分的，即存在一个超平面  wTx+b =0  将两类样本分开，那么对于每个样本都有y(n)(wTx(n) +b) > 0。 

​	我们定义整个数据集D中所有样本到分割超平面的最短距离为间隔（Margin），用γ 来表示。如果间隔γ 越大，其分割超平面对两个数据集的划分越稳定，不容易受噪声 等因素影响。支持向量机的目标是寻找一个超平面(w∗,b∗)使得γ 最大。

（2）支持向量（满足y(n)(wTx(n) +b) =1的样本）

​	![1572266086203](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1572266086203.png)



​	支持向量机的目标函数可以通过SMO等优化方法得到全局最优解，因此比其它分类器的学习效率更高。此外，支持向量机的决策函数只依赖于支持向量， 与训练样本总数无关，分类速度比较快。 

（3）支持向量机还有一个重要的优点是可以使用核函数（Kernel Function）隐式地将样本从原始特征空间映射到更**高维的空间**，并解决原始特征空间中的线性不可分问题。

​	常见的形式是两个不同样本向量的乘积

（4）软间隔

​	在支持向量机的优化问题中，约束条件比较严格。如果训练集中的样本在特 征空间中不是线性可分的，就无法找到最优解。

​	为了能够容忍部分不满足约束的样本（部分进入到间隔带中的样本），我们可以引入松弛变量ξ，引入松弛变量的间隔称为软间隔（Soft Margin）。

​	此时支持向量为满足y(n)(wTx(n) +b)+ξn =1的样本

